"""Request handler for content extraction."""

from __future__ import annotations

import hashlib
from datetime import datetime, timezone
from typing import Any

from apify import Actor
from crawlee.crawlers import PlaywrightCrawlingContext

from .extraction import (
    compute_content_info,
    extract_format,
    extract_metadata,
    get_extraction_options,
    save_content_to_kvs,
)


class ResultsCounter:
    """Thread-safe counter for tracking results."""

    def __init__(self, max_results: int) -> None:
        self.max_results = max_results
        self.count = 0

    def increment(self) -> int:
        """Increment and return new count."""
        self.count += 1
        return self.count

    def is_limit_reached(self) -> bool:
        """Check if max results limit is reached."""
        return self.max_results > 0 and self.count >= self.max_results


def create_request_handler(
    kvs: Any,
    dataset: Any | None,
    results_counter: ResultsCounter,
    browser_log_enabled: bool,
):
    """Create a request handler function.

    Args:
        kvs: Key-value store for content.
        dataset: Optional named dataset.
        results_counter: Counter for tracking results.
        browser_log_enabled: Whether to log browser console.

    Returns:
        Async handler function for PlaywrightCrawler.
    """

    async def handler(context: PlaywrightCrawlingContext) -> None:
        """Process a single page and extract content."""
        # Check if max results reached
        if results_counter.is_limit_reached():
            Actor.log.info(f'Max results ({results_counter.max_results}) reached, stopping')
            return

        url = context.request.url
        Actor.log.info(f'Processing {url}')

        # Enable browser console logging if requested
        if browser_log_enabled:
            context.page.on(
                'console',
                lambda msg: Actor.log.info(f'[Browser] {msg.type}: {msg.text}'),
            )

        html = await context.page.content()
        key_base = hashlib.md5(url.encode()).hexdigest()[:16]

        handler_config = context.request.user_data.get('config', {})
        extraction_mode = handler_config.get('extraction_mode', 'BALANCED')
        extract_opts = get_extraction_options(url, extraction_mode)

        # Build raw HTML info
        html_bytes = html.encode('utf-8')
        raw_html_info = compute_content_info(html_bytes)

        if handler_config.get('save_raw_html'):
            html_key = f'{key_base}-raw.html'
            await kvs.set_value(html_key, html, content_type='text/html; charset=utf-8')
            raw_html_info['key'] = html_key
            raw_html_info['url'] = await kvs.get_public_url(html_key)

        # Extract metadata
        metadata = extract_metadata(html, url)

        # Build dataset entry
        data: dict[str, Any] = {
            'loadedUrl': url,
            'rawHtml': raw_html_info,
            'loadedAt': datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
            'metadata': metadata,
            'httpStatus': 200,
        }

        # Save extracted formats
        await _save_extracted_formats(kvs, key_base, html, extract_opts, handler_config, data)

        # Push data to dataset
        if dataset:
            await dataset.push_data(data)
        else:
            await context.push_data(data)

        # Increment results count
        results_counter.increment()

        # Stop crawler if max results reached
        if results_counter.is_limit_reached():
            Actor.log.info(
                f'Max results ({results_counter.max_results}) reached, stopping crawler'
            )
            raise SystemExit(0)

        # Enqueue links if linkSelector is set
        await _enqueue_links(context, handler_config)

    return handler


async def _save_extracted_formats(
    kvs: Any,
    key_base: str,
    html: str,
    extract_opts: dict[str, Any],
    config: dict[str, Any],
    data: dict[str, Any],
) -> None:
    """Save extracted content in requested formats.

    Args:
        kvs: Key-value store.
        key_base: Base key for storage.
        html: Raw HTML content.
        extract_opts: Trafilatura options.
        config: Handler configuration.
        data: Data dict to update with results.
    """
    format_configs = [
        ('save_text', 'txt', 'extractedText', 'text/plain; charset=utf-8'),
        ('save_json', 'json', 'extractedJson', 'application/json; charset=utf-8'),
        ('save_markdown', 'markdown', 'extractedMarkdown', 'text/markdown; charset=utf-8'),
        ('save_xml', 'xml', 'extractedXml', 'application/xml; charset=utf-8'),
        ('save_xmltei', 'xmltei', 'extractedXmlTei', 'application/xml; charset=utf-8'),
    ]

    for config_key, output_format, data_key, content_type in format_configs:
        if config.get(config_key):
            content = extract_format(html, output_format, extract_opts)
            if content:
                ext = 'tei.xml' if output_format == 'xmltei' else output_format
                if output_format == 'markdown':
                    ext = 'md'
                key = f'{key_base}.{ext}'
                data[data_key] = await save_content_to_kvs(kvs, key, content, content_type)


async def _enqueue_links(
    context: PlaywrightCrawlingContext,
    config: dict[str, Any],
) -> None:
    """Enqueue links from the page if configured.

    Args:
        context: Crawling context.
        config: Handler configuration.
    """
    link_selector = config.get('link_selector', '')
    if not link_selector:
        return

    current_depth = context.request.user_data.get('depth', 0)
    max_depth = config.get('max_crawling_depth', 0)

    if max_depth != 0 and current_depth >= max_depth:
        return

    globs = config.get('globs', [])
    excludes = config.get('excludes', [])
    new_config = {**config}
    new_depth = current_depth + 1

    await context.enqueue_links(
        selector=link_selector,
        globs=[g.get('glob') for g in globs if g.get('glob')] if globs else None,
        exclude_globs=[e.get('glob') for e in excludes if e.get('glob')] if excludes else None,
        keep_url_fragments=config.get('keep_url_fragments', False),
        user_data={'config': new_config, 'depth': new_depth},
    )
